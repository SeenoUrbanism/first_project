{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b67f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "BASE_URL = \"https://cityjobs.nyc.gov/jobs?options=&page={}\"\n",
    "JOB_BASE_URL = \"https://cityjobs.nyc.gov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40cf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_job_links(listing_url):\n",
    "    \"\"\"Extracts job detail page links from a listing page.\"\"\"\n",
    "    res = requests.get(listing_url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    # Find job links (update selector as needed)\n",
    "    links = []\n",
    "    for a in soup.select('a[href^=\"/job/\"]'):\n",
    "        href = a['href']\n",
    "        if href.startswith(\"/job/\"):\n",
    "            links.append(JOB_BASE_URL + href)\n",
    "    return links\n",
    "\n",
    "def parse_job_detail(job_url):\n",
    "    \"\"\"Extracts required fields from a job detail page.\"\"\"\n",
    "    res = requests.get(job_url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    # Helper function to find labeled field text\n",
    "    def get_text_by_label(label):\n",
    "        tag = soup.find(text=lambda t: t and label in t)\n",
    "        if tag:\n",
    "            next_tag = tag.parent.find_next_sibling(text=True)\n",
    "            if next_tag:\n",
    "                return next_tag.strip()\n",
    "        return \"\"\n",
    "    # Business Title\n",
    "    title_tag = soup.find('h1')\n",
    "    business_title = title_tag.text.strip() if title_tag else \"\"\n",
    "    # Agency\n",
    "    agency = get_text_by_label(\"DEPARTMENT\")\n",
    "    # Posting Date\n",
    "    posting_date = get_text_by_label(\"Posted On:\")\n",
    "    # Preferred Skills (may need to search in Job Description section)\n",
    "    preferred_skills = \"\"\n",
    "    job_desc = soup.find('div', {'id': 'job-description'})\n",
    "    if job_desc:\n",
    "        ps_tag = job_desc.find(text=lambda t: \"Preferred Skills\" in t)\n",
    "        if ps_tag:\n",
    "            preferred_skills = ps_tag.parent.find_next_sibling('p').text.strip()\n",
    "    # Number of Positions\n",
    "    num_positions = get_text_by_label(\"Number of positions:\")\n",
    "    # Career Level\n",
    "    career_level = get_text_by_label(\"Experience level:\")\n",
    "    # Level\n",
    "    level = get_text_by_label(\"Job level\")\n",
    "    # Post Until\n",
    "    post_until = get_text_by_label(\"Posted until\")\n",
    "    # Job Category\n",
    "    job_category = get_text_by_label(\"Category:\")\n",
    "    # Return as dict\n",
    "    return {\n",
    "        'business_title': business_title,\n",
    "        'agency': agency,\n",
    "        'posting_date': posting_date,\n",
    "        'preferred_skills': preferred_skills,\n",
    "        '#_of_positions': num_positions,\n",
    "        'career_level': career_level,\n",
    "        'level': level,\n",
    "        'post_until': post_until,\n",
    "        'job_category': job_category,\n",
    "        'job_url': job_url\n",
    "    }\n",
    "\n",
    "def scrape_city_jobs(max_pages=5):\n",
    "    \"\"\"Scrapes multiple pages of NYC job postings.\"\"\"\n",
    "    jobs = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        listing_url = BASE_URL.format(page)\n",
    "        job_links = get_job_links(listing_url)\n",
    "        print(f\"Scraping page {page}, found {len(job_links)} jobs.\")\n",
    "        for job_url in job_links:\n",
    "            try:\n",
    "                job_info = parse_job_detail(job_url)\n",
    "                jobs.append(job_info)\n",
    "                time.sleep(0.5)  # Be polite!\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping job: {job_url} - {e}\")\n",
    "    return pd.DataFrame(jobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdfc086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1, found 0 jobs.\n",
      "Scraping page 2, found 0 jobs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Run scraper for first N pages ---\n",
    "df_jobs = scrape_city_jobs(max_pages=2)\n",
    "df_jobs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
